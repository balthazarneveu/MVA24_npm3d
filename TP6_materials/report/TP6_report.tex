\documentclass[a4paper]{article} 
\input{style/head.tex}

%-------------------------------
%	TITLE VARIABLES (identify your work!)
%-------------------------------

\newcommand{\yourname}{Balthazar Neveu | Jamy Lafenetre}
\newcommand{\youremail}{balthazarneveu@gmail.com | jamy.lafenetre@ens-paris-saclay.fr}
\newcommand{\assignmentnumber}{6}

\begin{document}

%-------------------------------
%	TITLE SECTION (do not modify unless you really need to)
%-------------------------------
\input{style/header.tex}


%-------------------------------
%	ASSIGNMENT CONTENT (add your responses)
%-------------------------------

\section*{Preliminary question: Ability of MLP to classify coordinates.}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mlp_manual.png}
    \caption{Multi layer perceptron show the ability to  classify coordinates. We display parameters which led to good results on the spiral classification.}
    \label{fig:MPL_search}
\end{figure}

\begin{itemize}
    \item The Cybenko theorem of universal approximation states that we should be able to classify this point cloud with a single hidden layer with enough neurons. 
    \item When we use ReLu function, MLP outputs a piecewise linear function of the input coordinates.
    \item To classify the spiral points, the output logits may need to use the radius... which is possible to approximate with a very wide hidden layer.
    \item Hower instead of learning to appoximate $x^2 + y^2$ with a bunch of layers and ReLU to get the radius, we can directly inject it as an extra input.
    \item Introducing squared coordinates (and sinus embeddings) allows the model to output piecewise linear functions of more complex inputs features (a piecewise quadratic form if we consider the squared $x^2$ ).
    \item We note that finding the right architecture (depth and number of layers) is a matter of trial and error here.
\end{itemize}

We can see that eventually, we're able to find correct parameters which allow finding a shape which roughly match the spiral.   

Now the following lab session deals on how we classify 3D point clouds (e.g. unordered sets of points in 3D space). We will use a PointNet architecture to classify 3D point clouds.

\section*{Question 1}

We train our model on ModelNet40. It has 1.708M parameters. Using a learning rate of $10^{-}2$, we
are only able to reach an accuracy of $12\%$ after 75 epochs (convergence is achieved).

The accuracy of a random classifier would be $2.5\%$, so this result is not dramatic. 
However it cannot be considered a good classifier.
Please note that MLP considers that the order of the points matters (a list instead of a set) and therefore the output is not \textbf{permutation invariant}.
A detailed unitary test is available in the code (test\_pointnet.py to make sure that pointNet architecture is actually permutation invariant).


\section*{Question 2}
On ModelNet40, we reach an accuracy of $86\%$ after 75 epochs using a learning rate of $1e-3$.
These results are considerably higher than using a simple MLP. A Pointnet without Tnets can be used
like a satisfactory classifier.
PointNet processes each point of the set separately until the max pooling operator is reached. Max pooling statisfies the permutation invariance.


\section*{Question 3}
On ModelNet40, we reach an accuracy of $84\%$ after 100 epochs using a learning rate of $5e-3$.
These results are slightly under what could be obtained without Tnet.
Our intuition is that by using TNet, network may learn to orient objects in a canonical way, which may not always the best way to classify them... it is almost like trying to counter-act the effect of augmentations.

\section*{Question 4}
We propose two sources of data augmentation. 
\begin{itemize}
    \item First, we apply three random scaling factors centered around 1 in the x, y and z direction. 
    \item Second, we remove a random proportion of the points (around $10\%$) and replace them by duplicating other points.
\end{itemize}
Using the same parameters as before, the impact of this data augmentation is not meaningful and seems to slightly hinder the accuracy.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Q4.png}
    \caption{Impact on validation accuracy of our augmentations during PointNet training - seems like adding new augmentations sligtlty slowed down convergence and prevented to reach the best accuracy.}
    \label{fig:pointNet augmentations}
\end{figure}

\section*{Conclusion}
All evaluations conducted on ModelNet40 are summarized in Table \ref{tab:classification_results}. We can see that the PointNet architecture is able to classify 3D point clouds with a decent accuracy. However, the use of T-Net (promising idea on paper) does not seem to improve the results... nor data augmentation.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Learning Rate} & \textbf{Epochs} & \textbf{Accuracy} \\ \hline
    MLP (Not permutation invariant) & $10^{-2}$ & 75 & 12\% \\ \hline
    PointNet without T-Net & $10^{-3}$ & 75 & 86\% \\ \hline
    PointNet with T-Net & $5 \times 10^{-3}$ & 100 & 84\% \\ \hline
    PointNet with T-Net and Data Augmentation & $5. 10^{-3}$ & 100 & 83\% \\ \hline
    \end{tabular}
    \caption{Summary of Point Cloud Classification Results}
    \label{tab:classification_results}
\end{table}

\section*{Appendix: training curves}

    
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Q1.png}
    \caption{MLP accuracy during training}
    \label{fig:MLP}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Q3.png}
    \caption{PointNet (no TNET vs TNET) + standard augmentations) validation accuracy during training}
    \label{fig:pointNet}
\end{figure}





\end{document}



